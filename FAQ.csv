id,questions,answers
0,Do you have a paper or other written explanation to introduce your model's details?,The design philosophy and technical details can be found in my blog post.
1,Where is the BERT code come from?,"BERT code of this repo is forked from the original BERT repo with necessary modification, especially in extract_features.py."
2,How large is a sentence vector?,"In general, each sentence is translated to a 768-dimensional vector. Depending on the pretrained BERT you are using, pooling_strategy and pooling_layer the dimensions of the output vector could be different."
3,How do you get the fixed representation? Did you do pooling or something?,"Yes, pooling is required to get a fixed representation of a sentence. In the default strategy REDUCE_MEAN, I take the second-to-last hidden layer of all of the tokens in the sentence and do average pooling."
4,Are you suggesting using BERT without fine-tuning?,"Yes and no. On the one hand, Google pretrained BERT on Wikipedia data, thus should encode enough prior knowledge of the language into the model. Having such feature is not a bad idea. On the other hand, these prior knowledge is not specific to any particular domain. It should be totally reasonable if the performance is not ideal if you are using it on, for example, classifying legal cases. Nonetheless, you can always first fine-tune your own BERT on the downstream task and then use bert-as-service to extract the feature vectors efficiently. Keep in mind that bert-as-service is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT."
5,Can I get a concatenation of several layers instead of a single layer ?,Sure! Just use a list of the layer you want to concatenate when calling the server. Example:
6,What are the available pooling strategies?,Here is a table summarizes all pooling strategies I implemented. Choose your favorite one by specifying bert_serving_start -pooling_strategy.
7,"Why not use the hidden state of the first token as default strategy, i.e. the [CLS]?","Because a pre-trained model is not fine-tuned on any downstream tasks yet. In this case, the hidden state of [CLS] is not a good sentence representation. If later you fine-tune the model, you may use [CLS] as well."
8,"BERT has 12/24 layers, so which layer are you talking about?","By default this service works on the second last layer, i.e. pooling_layer=-2. You can change it by setting pooling_layer to other negative values, e.g. -1 corresponds to the last layer."
9,Why not the last hidden layer? Why second-to-last?,"The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets. If you question about this argument and want to use the last hidden layer anyway, please feel free to set pooling_layer=-1."
10,So which layer and which pooling strategy is the best?,"It depends. Keep in mind that different BERT layers capture different information. To see that more clearly, here is a visualization on UCI-News Aggregator Dataset, where I randomly sample 20K news titles; get sentence encodes from different layers and with different pooling strategies, finally reduce it to 2D via PCA (one can of course do t-SNE as well, but that's not my point). There are only four classes of the data, illustrated in red, blue, yellow and green. To reproduce the result, please run example7.py."
11,Could I use other pooling techniques?,"For sure. But if you introduce new tf.variables to the graph, then you need to train those variables before using the model. You may also want to check some pooling techniques I mentioned in my blog post."
12,Do I need to batch the data before encode()?,"No, not at all. Just do encode and let the server handles the rest. If the batch is too large, the server will do batching automatically and it is more efficient than doing it by yourself. No matter how many sentences you have, 10K or 100K, as long as you can hold it in client's memory, just send it to the server. Please also read the benchmark on the client batch size."
13,Can I start multiple clients and send requests to one server simultaneously?,Yes! That's the purpose of this repo. In fact you can start as many clients as you want. One server can handle all of them (given enough time).
14,How many requests can one service handle concurrently?,"The maximum number of concurrent requests is determined by num_worker in bert_serving_start. If you a sending more than num_worker requests concurrently, the new requests will be temporally stored in a queue until a free worker becomes available."
15,So one request means one sentence?,"No. One request means a list of sentences sent from a client. Think the size of a request as the batch size. A request may contain 256, 512 or 1024 sentences. The optimal size of a request is often determined empirically. One large request can certainly improve the GPU utilization, yet it also increases the overhead of transmission. You may run python example/example1.py for a simple benchmark."
16,How about the speed? Is it fast enough for production?,"It highly depends on the max_seq_len and the size of a request. On a single Tesla M40 24GB with max_seq_len=40, you should get about 470 samples per second using a 12-layer BERT. In general, I'd suggest smaller max_seq_len (25) and larger request size (512/1024)."
17,Did you benchmark the efficiency?,Yes. See Benchmark.
18,What is backend based on?,ZeroMQ.
19,What is the parallel processing model behind the scene?,
20,Why does the server need two ports?,"One port is for pushing text data into the server, the other port is for publishing the encoded result to the client(s). In this way, we get rid of back-chatter, meaning that at every level recipients never talk back to senders. The overall message flow is strictly one-way, as depicted in the above figure. Killing back-chatter is essential to real scalability, allowing us to use BertClient in an asynchronous way."
21,Do I need Tensorflow on the client side?,"No. Think of BertClient as a general feature extractor, whose output can be fed to any ML models, e.g. scikit-learn, pytorch, tensorflow. The only file that client need is client.py. Copy this file to your project and import it, then you are ready to go."
22,Can I use multilingual BERT model provided by Google?,Yes.
23,Can I use my own fine-tuned BERT model?,"Yes. In fact, this is suggested. Make sure you have the following three items in model_dir:"
24,Can I run it in python 2?,"Server side no, client side yes. This is based on the consideration that python 2.x might still be a major piece in some tech stack. Migrating the whole downstream stack to python 3 for supporting bert-as-service can take quite some effort. On the other hand, setting up BertServer is just a one-time thing, which can be even run in a docker container. To ease the integration, we support python 2 on the client side so that you can directly use BertClient as a part of your python 2 project, whereas the server side should always be hosted with python 3."
25,Do I need to do segmentation for Chinese?,"No, if you are using the pretrained Chinese BERT released by Google you don't need word segmentation. As this Chinese BERT is character-based model. It won't recognize word/phrase even if you intentionally add space in-between. To see that more clearly, this is what the BERT model actually receives after tokenization:"
26,Why my (English) word is tokenized to ##something?,Because your word is out-of-vocabulary (OOV). The tokenizer from Google uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary.
27,Can I use my own tokenizer?,"Yes. If you already tokenize the sentence on your own, simply send use encode with List[List[Str]] as input and turn on is_tokenized, i.e. bc.encode(texts, is_tokenized=True)."
28,"I encounter zmq.error.ZMQError: Operation cannot be accomplished in current state when using BertClient, what should I do?","This is often due to the misuse of BertClient in multi-thread/process environment. Note that you canâ€™t reuse one BertClient among multiple threads/processes, you have to make a separate instance for each thread/process. For example, the following won't work at all:"
29,"After running the server, I have several garbage tmpXXXX folders. How can I change this behavior ?","These folders are used by ZeroMQ to store sockets. You can choose a different location by setting the environment variable ZEROMQ_SOCK_TMP_DIR :
export ZEROMQ_SOCK_TMP_DIR=/tmp/"
30,"The cosine similarity of two sentence vectors is unreasonably high (e.g. always > 0.8), what's wrong?","A decent representation for a downstream task doesn't mean that it will be meaningful in terms of cosine distance. Since cosine distance is a linear space where all dimensions are weighted equally. if you want to use cosine distance anyway, then please focus on the rank not the absolute value. Namely, do not use:"
31,"I'm getting bad performance, what should I do?","This often suggests that the pretrained BERT could not generate a descent representation of your downstream task. Thus, you can fine-tune the model on the downstream task and then use bert-as-service to serve the fine-tuned BERT. Note that, bert-as-service is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT."
32,Can I run the server side on CPU-only machine?,"Yes, please run bert-serving-start -cpu -max_batch_size 16. Note that, CPU does not scale as good as GPU on large batches, therefore the max_batch_size on the server side needs to be smaller, e.g. 16 or 32."
33,How can I choose num_worker?,"Generally, the number of workers should be less than or equal to the number of GPU/CPU you have. Otherwise, multiple workers will be allocated to one GPU/CPU, which may not scale well (and may cause out-of-memory on GPU)."
34,Can I specify which GPU to use?,"Yes, you can specifying -device_map as follows:"
